@article{SchweitzerPASCAL2011,
abstract = {In this article, we propose a family of efficient kernels for large graphs with discrete node la-bels. Key to our method is a rapid feature extraction scheme based on the Weisfeiler-Lehman test of isomorphism on graphs. It maps the original graph to a sequence of graphs, whose node at-tributes capture topological and label information. A family of kernels can be defined based on this Weisfeiler-Lehman sequence of graphs, including a highly efficient kernel comparing subtree-like patterns. Its runtime scales only linearly in the number of edges of the graphs and the length of the Weisfeiler-Lehman graph sequence. In our experimental evaluation, our kernels outperform state-of-the-art graph kernels on several graph classification benchmark data sets in terms of accu-racy and runtime. Our kernels open the door to large-scale applications of graph kernels in various disciplines such as computational biology and social network analysis.},
author = {Shervashidze, Nino and Schweitzer, Pascal and {Jan van Leeuwen}, Erik and Mehlhorn, Kurt and Borgwardt, Karsten M},
doi = {10.1.1.232.1510},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/Schweitzer PASCAL et al. - 2011 - Weisfeiler-Lehman Graph Kernels Nino Shervashidze Kurt Mehlhorn Karsten M. Borgwardt.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {Weisfeiler-Lehman algorithm,graph classification,graph kernels,similarity measures for graphs},
pages = {2539--2561},
pmid = {17324400},
title = {{Weisfeiler-Lehman Graph Kernels}},
url = {http://www.jmlr.org/papers/volume12/shervashidze11a/shervashidze11a.pdf},
volume = {12},
year = {2011}
}
@article{McInnes2018,
abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
archivePrefix = {arXiv},
arxivId = {1802.03426},
author = {McInnes, Leland and Healy, John and Melville, James},
eprint = {1802.03426},
file = {::},
month = {feb},
title = {{UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction}},
url = {http://arxiv.org/abs/1802.03426},
year = {2018}
}
@article{Le2014,
abstract = {Many machine learning algorithms require the input to be represented as a fixed-length feature vector. When it comes to texts, one of the most common fixed-length features is bag-of-words. Despite their popularity, bag-of-words features have two major weaknesses: they lose the ordering of the words and they also ignore semantics of the words. For example, "powerful," "strong" and "Paris" are equally distant. In this paper, we propose Paragraph Vector, an unsupervised algorithm that learns fixed-length feature representations from variable-length pieces of texts, such as sentences, paragraphs, and documents. Our algorithm represents each document by a dense vector which is trained to predict words in the document. Its construction gives our algorithm the potential to overcome the weaknesses of bag-of-words models. Empirical results show that Paragraph Vectors outperform bag-of-words models as well as other techniques for text representations. Finally, we achieve new state-of-the-art results on several text classification and sentiment analysis tasks.},
archivePrefix = {arXiv},
arxivId = {1405.4053},
author = {Le, Quoc V. and Mikolov, Tomas},
eprint = {1405.4053},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/Le, Mikolov - 2014 - Distributed Representations of Sentences and Documents(4).pdf:pdf},
month = {may},
title = {{Distributed Representations of Sentences and Documents}},
url = {http://arxiv.org/abs/1405.4053},
year = {2014}
}
@article{Lagunin2000,
author = {Lagunin, A. and Stepanchikova, A. and Filimonov, D. and Poroikov, V.},
doi = {10.1093/bioinformatics/16.8.747},
file = {::},
issn = {1367-4803},
journal = {Bioinformatics},
month = {aug},
number = {8},
pages = {747--748},
publisher = {Oxford University Press},
title = {{PASS: prediction of activity spectra for biologically active substances}},
url = {https://academic.oup.com/bioinformatics/article-lookup/doi/10.1093/bioinformatics/16.8.747},
volume = {16},
year = {2000}
}
@techreport{Duvenaud,
abstract = {We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better pre-dictive performance on a variety of tasks.},
author = {Duvenaud, David and Maclaurin, Dougal and Aguilera-Iparraguirre, Jorge and G{\'{o}}mez-Bombarelli, Rafael and Hirzel, Timothy and Aspuru-Guzik, Al{\'{a}}n and Adams, Ryan P},
file = {::},
title = {{Convolutional Networks on Graphs for Learning Molecular Fingerprints}},
url = {http://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.pdf}
}
@article{Narayanan,
abstract = {Recent works on representation learning for graph struc-tured data predominantly focus on learning distributed representations of graph substructures such as nodes and sub-graphs. However, many graph analytics tasks such as graph classification and clustering require representing entire graphs as fixed length feature vectors. While the afore-mentioned approaches are naturally unequipped to learn such representations, graph kernels remain as the most effective way of obtaining them. However, these graph kernels use handcrafted features (e.g., shortest paths, graphlets, etc.) and hence are hampered by problems such as poor generalization. To address this limitation, in this work, we propose a neural embedding framework named graph2vec to learn data-driven distributed representations of arbitrary sized graphs. graph2vec's embeddings are learnt in an un-supervised manner and are task agnostic. Hence, they could be used for any downstream task such as graph classification, clustering and even seeding supervised representation learning approaches. Our experiments on several benchmark and large real-world datasets show that graph2vec achieves significant improvements in classification and clustering accuracies over substructure representation learning approaches and are competitive with state-of-the-art graph kernels.},
archivePrefix = {arXiv},
arxivId = {1707.05005v1},
author = {Narayanan, Annamalai and Chandramohan, Mahinthan and Venkatesan, Rajasekar and Chen, Lihui and Liu, Yang and Jaiswal, Shantanu},
doi = {10.1145/1235},
eprint = {1707.05005v1},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/Narayanan et al. - Unknown - graph2vec Learning Distributed Representations of Graphs.pdf:pdf},
isbn = {9781450321389},
keywords = {Deep Learning,Graph Kernels,Representation Learning},
title = {{graph2vec: Learning Distributed Representations of Graphs}},
url = {https://arxiv.org/pdf/1707.05005.pdf}
}
@article{Brookes2018,
abstract = {We present a probabilistic modeling framework and adaptive sampling algorithm wherein unsupervised generative models are combined with black box predictive models to tackle the problem of input design. In input design, one is given one or more stochastic "oracle" predictive functions, each of which maps from the input design space (e.g. DNA sequences or images) to a distribution over a property of interest (e.g. protein fluorescence or image content). Given such stochastic oracles, the problem is to find an input that is expected to maximize one or more properties, or to achieve a specified value of one or more properties, or any combination thereof. We demonstrate experimentally that our approach substantially outperforms other recently presented methods for tackling a specific version of this problem, namely, maximization when the oracle is assumed to be deterministic and unbiased. We also demonstrate that our method can tackle more general versions of the problem.},
archivePrefix = {arXiv},
arxivId = {1810.03714},
author = {Brookes, David H. and Listgarten, Jennifer},
eprint = {1810.03714},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/Brookes, Listgarten - 2018 - Design by adaptive sampling.pdf:pdf},
month = {oct},
title = {{Design by adaptive sampling}},
url = {http://arxiv.org/abs/1810.03714},
year = {2018}
}
@techreport{Lamb,
abstract = {To pursue a systematic approach to the discovery of functional connections among diseases, genetic perturbation, and drug action, we have created the first installment of a reference collection of gene-expression profiles from cultured human cells treated with bioactive small molecules, together with pattern-matching software to mine these data. We demonstrate that this ''Connectivity Map'' resource can be used to find connections among small molecules sharing a mechanism of action, chemicals and physiological processes, and diseases and drugs. These results indicate the feasibility of the approach and suggest the value of a large-scale community Connectivity Map project.},
author = {Lamb, Justin and Crawford, Emily D and Peck, David and Modell, Joshua W and Blat, Irene C and Wrobel, Matthew J and Lerner, Jim and Brunet, Jean-Philippe and Subramanian, Aravind and Ross, Kenneth N and Reich, Michael and Hieronymus, Haley and Wei, Guo and Armstrong, Scott A and Haggarty, Stephen J and Clemons, Paul A and Wei, Ru and Carr, Steven A and Lander, Eric S and Golub, Todd R},
file = {::},
title = {{The Connectivity Map: Using Gene-Expression Signatures to Connect Small Molecules, Genes, and Disease}},
url = {http://science.sciencemag.org/}
}
@article{Kang2016,
abstract = {A method to systematically identify optimal biomarkers improves high-content screening for drug candidates.},
author = {Kang, Jungseog and Hsu, Chien-Hsiang and Wu, Qi and Liu, Shanshan and Coster, Adam D and Posner, Bruce A and Altschuler, Steven J and Wu, Lani F},
doi = {10.1038/nbt.3419},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/Kang et al. - 2016 - Improving drug discovery with high-content phenotypic screens by systematic selection of reporter cell lines(3).pdf:pdf},
issn = {1087-0156},
journal = {Nature Biotechnology},
keywords = {High,Image processing,throughput screening},
month = {jan},
number = {1},
pages = {70--77},
publisher = {Nature Publishing Group},
title = {{Improving drug discovery with high-content phenotypic screens by systematic selection of reporter cell lines}},
url = {http://www.nature.com/articles/nbt.3419},
volume = {34},
year = {2016}
}
@article{Gomez-Bombarelli2018,
abstract = {We report a method to convert discrete representations of molecules to and from a multidimensional continuous representation. This model allows us to generate new molecules for efficient exploration and optimization through open-ended spaces of chemical compounds. A deep neural network was trained on hundreds of thousands of existing chemical structures to construct three coupled functions: an encoder, a decoder and a predictor. The encoder converts the discrete representation of a molecule into a real-valued continuous vector, and the decoder converts these continuous vectors back to discrete molecular representations. The predictor estimates chemical properties from the latent continuous vector representation of the molecule. Continuous representations allow us to automatically generate novel chemical structures by performing simple operations in the latent space, such as decoding random vectors, perturbing known chemical structures, or interpolating between molecules. Continuous representations also allow the use of powerful gradient-based optimization to efficiently guide the search for optimized functional compounds. We demonstrate our method in the domain of drug-like molecules and also in the set of molecules with fewer that nine heavy atoms.},
archivePrefix = {arXiv},
arxivId = {1610.02415},
author = {G{\'{o}}mez-Bombarelli, Rafael and Wei, Jennifer N. and Duvenaud, David and Hern{\'{a}}ndez-Lobato, Jos{\'{e}} Miguel and S{\'{a}}nchez-Lengeling, Benjam{\'{i}}n and Sheberla, Dennis and Aguilera-Iparraguirre, Jorge and Hirzel, Timothy D. and Adams, Ryan P. and Aspuru-Guzik, Al{\'{a}}n},
doi = {10.1021/acscentsci.7b00572},
eprint = {1610.02415},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/chemvae.pdf:pdf},
isbn = {8659122866135},
issn = {23747951},
journal = {ACS Central Science},
month = {feb},
number = {2},
pages = {268--276},
pmid = {22614679},
publisher = {American Chemical Society},
title = {{Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules}},
url = {http://pubs.acs.org/doi/10.1021/acscentsci.7b00572},
volume = {4},
year = {2018}
}
@techreport{Fukuyama2017,
abstract = {When working with large biological data sets, exploratory analysis is an important first step for understanding the latent structure and for generating hypotheses to be tested in subsequent analyses. However, when the number of variables is large compared to the number of samples, standard methods such as principal components analysis give results which are unstable and difficult to interpret. To mitigate these problems, we have developed a method which allows the analyst to incorporate side information about the relationships between the variables in a way that encourages similar variables to have similar loadings on the principal axes. This leads to a low-dimensional representation of the samples which both describes the latent structure and which has axes which are interpretable in terms of groups of closely related variables. The method is derived by putting a prior encoding the relationships between the variables on the data and following through the analysis on the posterior distributions of the samples. We show that our method does well at reconstructing true latent structure in simulated data and we also demonstrate the method on a dataset investigating the effects of antibiotics on the composition of bacteria in the human gut.},
archivePrefix = {arXiv},
arxivId = {arXiv:1702.00501v1},
author = {Fukuyama, Julia},
eprint = {arXiv:1702.00501v1},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/Fukuyama - 2017 - Adaptive gPCA A method for structured dimensionality reduction(2).pdf:pdf},
title = {{Adaptive gPCA: A method for structured dimensionality reduction}},
url = {https://arxiv.org/pdf/1702.00501.pdf},
year = {2017}
}
@article{Goldberg2014,
abstract = {The word2vec software of Tomas Mikolov and colleagues 1 has gained a lot of traction lately, and provides state-of-the-art word embeddings. The learning models behind the software are described in two research papers [1, 2]. We found the description of the models in these papers to be somewhat cryptic and hard to follow. While the motivations and presentation may be obvious to the neural-networks language-modeling crowd, we had to struggle quite a bit to figure out the rationale behind the equations. This note is an attempt to explain equation (4) (negative sampling) in " Dis-tributed Representations of Words and Phrases and their Compositionality " by The departure point of the paper is the skip-gram model. In this model we are given a corpus of words w and their contexts c. We consider the conditional probabilities p(c|w), and given a corpus T ext, the goal is to set the parameters $\theta$ of p(c|w; $\theta$) so as to maximize the corpus probability: arg max},
archivePrefix = {arXiv},
arxivId = {arXiv:1402.3722v1},
author = {Goldberg, Yoav and Levy, Omer and Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {arXiv:1402.3722v1},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/Goldberg et al. - 2014 - word2vec Explained Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method The skip-gram model.pdf:pdf},
keywords = {()},
title = {{word2vec Explained: Deriving Mikolov et al.'s Negative-Sampling Word-Embedding Method The skip-gram model}},
url = {https://arxiv.org/pdf/1402.3722.pdf},
year = {2014}
}
@article{Hamid2018,
abstract = {Antibiotic resistance is a major public health crisis, and finding new sources of antimicrobial drugs is crucial to solving it. Bacteriocins, which are bacterially-produced antimicrobial peptide products, are candidates for broadening our pool of antimicrobials. The discovery of new bacteriocins by genomic mining is hampered by their sequences' low complexity and high variance, which frustrates sequence similarity-based searches. Here we use word embeddings of protein sequences to represent bacteriocins, and subsequently apply Recurrent Neural Networks and Support Vector Machines to predict novel bacteriocins from protein sequences without using sequence similarity. We developed a word embedding method that accounts for sequence order, providing a better classification than a simple summation of the same word embeddings. We use the Uniprot/TrEMBL database to acquire the word embeddings taking advantage of a large volume of unlabeled data. Our method predicts, with a high probability, six yet unknown putative bacteriocins in Lactobacillus. Generalized, the representation of sequences with word embeddings preserving sequence order information can be applied to protein classification problems for which sequence homology cannot be used.},
author = {Hamid, Md Nafiz and Friedberg, Iddo},
doi = {10.1101/255505},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/Hamid, Friedberg - 2018 - Identifying Antimicrobial Peptides using Word Embedding with Deep Recurrent Neural Networks.pdf:pdf},
journal = {bioRxiv},
month = {jan},
pages = {255505},
publisher = {Cold Spring Harbor Laboratory},
title = {{Identifying Antimicrobial Peptides using Word Embedding with Deep Recurrent Neural Networks}},
url = {https://www.biorxiv.org/content/early/2018/01/29/255505},
year = {2018}
}
@article{Asgari2015,
abstract = {We introduce a new representation and feature extraction method for biological sequences. Named bio-vectors (BioVec) to refer to biological sequences in general with protein-vectors (ProtVec) for proteins (amino-acid sequences) and gene-vectors (GeneVec) for gene sequences, this representation can be widely used in applications of deep learning in proteomics and genomics. In the present paper, we focus on protein-vectors that can be utilized in a wide array of bioinformatics investigations such as family classification, protein visualization, structure prediction, disordered protein identification, and protein-protein interaction prediction. In this method, we adopt artificial neural network approaches and represent a protein sequence with a single dense n-dimensional vector. To evaluate this method, we apply it in classification of 324,018 protein sequences obtained from Swiss-Prot belonging to 7,027 protein families, where an average family classification accuracy of 93{\%}+-0.06{\%} is obtained, outperforming existing family classification methods. In addition, we use ProtVec representation to predict disordered proteins from structured proteins. Two databases of disordered sequences are used: the DisProt database as well as a database featuring the disordered regions of nucleoporins rich with phenylalanine-glycine repeats (FG-Nups). Using support vector machine classifiers, FG-Nup sequences are distinguished from structured protein sequences found in Protein Data Bank (PDB) with a 99.8{\%} accuracy, and unstructured DisProt sequences are differentiated from structured DisProt sequences with 100.0{\%} accuracy. These results indicate that by only providing sequence data for various proteins into this model, accurate information about protein structure can be determined.},
archivePrefix = {arXiv},
arxivId = {1503.05140},
author = {Asgari, Ehsaneddin and Mofrad, Mohammad R.K.},
doi = {10.1371/journal.pone.0141287},
editor = {Kobeissy, Firas H},
eprint = {1503.05140},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/Asgari, Mofrad - 2015 - Continuous Distributed Representation of Biological Sequences for Deep Proteomics and Genomics.pdf:pdf},
isbn = {19326203 (Electronic)},
issn = {19326203},
journal = {PLoS ONE},
month = {nov},
number = {11},
pages = {e0141287},
pmid = {26555596},
publisher = {Public Library of Science},
title = {{Continuous distributed representation of biological sequences for deep proteomics and genomics}},
url = {http://dx.plos.org/10.1371/journal.pone.0141287},
volume = {10},
year = {2015}
}
@article{Mikolov,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {arXiv:1301.3781v3},
author = {Mikolov, Tomas and Corrado, Greg and Chen, Kai and Dean, Jeffrey},
doi = {10.1162/153244303322533223},
eprint = {arXiv:1301.3781v3},
file = {:Users/nthomas/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Proceedings of the International Conference on Learning Representations (ICLR 2013)},
pages = {1--12},
pmid = {18244602},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://arxiv.org/pdf/1301.3781v3.pdf},
year = {2013}
}
